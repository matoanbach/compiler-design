# Syntactical Analysis: Introduction
- Syntax analysis involves parsing the token sequence to identify the syntatic structure of the program.
- The parser's output is some form of intermediate representation of the program's structure, typically an abstract syntax tree, which replaces the linear sequence of tokens with a tree structure built according to the rules of a formal grammer which is used to define the language's syntax.
- This is usually done using a context-free grammaer which recursively defines syntatical structures that can make up a valid program and the order and combinations in which they must appear.
- The resulting abstract syntax tree is then analyzed, augmented, and transformed by later phases in the compiler
- Parsers can be written by hand or generated by parser generators, such as Yacc, Bison, ANTLR or JavaCC, among other tools.

## Syntatical analyzer: roles
- Roles
    - Analyze the structure of the program and its component
    - Check for, report, and recover from syntax errors.
    - Drive the front-end's execution.

## Syntax analysis: history
- Syntax analysis orginates from linguistics, especially Noamsky's genenerative grammer theory from the 1950s.
- A generative grammer defines how valid sentences (or programs) can be built using a set of production rules - starting from abstract symbols and refining them into specific lexical elements
- The parser uses these rules to construct parse trees, which visually represent how a sentence follows the grammar.
- The step-by-step application of rules that shows this process is called a derivation.
- Most parsing algorithms were developed in the 1960s, and Donal Knuth is credited for formalizing and popularizing many of them.

## Syntax and Semantics
- `Syntax`: defines how valid sentences are formed
- `Semantics`: defines the meaning of valid sentences
- Some grammatically correct sentences can have no meaning
- It is impossible to automatically validate the full meaning of all syntactically valid English sentences.
    - Spoken languages may have ambiguous meaning
    - Programming languages must be non-ambiguous
- In programming languages, semantics is about giving a meaning by translating programs into executables

## Grammars
- A grammar is represented as a 4-tuple (T, N, S, R):
    - T: the set of terminal symbols (actual tokens like if, +, ;)
    - N: the set of non-terminal symbols (syntatic variables like `<expr>` or `<stmt>`)
    - S: the start symbol (one of the non-terminals)
    - R: the et of production rules (e.g., A -> B) that describe how non-terminals can be replaced by terminals and other non-terminals.
- A context-free grammar (CFG) is one where each rule's left-hand side is a single non-terminal
- A sentential form is any intermediate string that still has non-terminals (e.g `<article><verb><noun>`)
- A sentence is a string made up only of terminals - meaning it's a complete, valid program structure (e.g., the dog gnawed the bone)

## Backus-Naur Form
- `J.W.Backus`: contributing designer of the first FORTRAN compiler
- `P.Naur`: contributing designer of the Algol-60 programming language
    - non-terminals are placed in angle brackets
    - the symbol `::=` is used instead of an arrow
    - a vertical bar can be used to signify `alternatives`
    - curly braces are used to signify an indefinite number of `repetitions`
    - square brackets are used to signify `optionality`
- Widely used to represent programming languages' syntax

## BNF: Example
- Pascal type declarations
```go
<typedecl>  ::= type <typedeflist>
<typedeflist> ::= <typedef> [ <typedeflist> ]
<typedef>   ::= <typeid> = <typespec> ;
<typespec>  ::= <typeid> 
             |  <arraydef>
             |  <ptrdef>
             |  <rangedef>
             |  <enumdef>
             |  <recdef>
<typeid>    ::= id
<arraydef>  ::= [ packed ] array <lbrack> <rangedef> <rbrack> of <typeid>
<lbrack>    ::= [
<rbrack>    ::= ]
<ptrdef>    ::= ^<typeid>
<rangedef>  ::= <number>..<number>
<number>    ::= <digit> [ <number> ]
<enumdef>   ::= <lparen> <idlist> <rparen>
<lparen>    ::= (
<rparen>    ::= )
<idlist>    ::= <ident> { , <ident>}
<recdef>    ::= record <vardecllist> end ;
<vardecllist>   ::= <vardecl> [ <vardecllist> ]
<vardecl>  ::= <idlist> : <typespec> ;
```
- Example
```txt
type string2- = packed array[1..20] of char;
type intptr = ^integer;
     floatptr = ^real;

type herb = (tarragon, rosemary, thyme, alpert);
     tinyint = 1..7;
     student = record
                    name, address : string20
                    studentid     : array[1..20] of integer;
                    strade        : char;
                end;;

```

### Example
- Grammar for simple arithmetic expressions:
```txt
G = (T, N, S, R),
T = {id,+,-,*,/,(,)},
N = {E},
S = E,
R = {
    E -> E + E,
    E -> E - E,
    E -> E * E,
    E -> E / E,
    E -> (E),
    E -> id
}
```

## Top-down parsing
- Starts at the root (starting symbol)
- Builds the tree downwards, while taking input from:
    - The sequence of tokens representing the program (from left to right)
    - The rules in the grammar specifying the programming language

## Derivations
- The application of grammar rules toward the recognition of a grammatically valid sequence of terminals can be represented with a derivation
- A derivation is noted as a series of transformations:
    - alpha => beta [p] where p is the production rule used to replace part of alpha with beta.
    - Each step replaces one non-terminal in the current sentential form (alpha) with what the grammar rule specifies, resulting in a new sentential form (beta)

## Top-down and bottom-up parsing
- A `top-down` parser builds a parse tree starting at the root down to the leafs
    - It builds leftmost derivations, i.e. a forward derivation proving that a sentence can be generated from the starting symbol by using a sequence of forward applications of productions.
- A `bottom-up` parser builds a parse tree starting from the leafs up to the root
    - It builds rightmost derivations, i.e. a reverse derivation proving that one can come to the starting symbol from a sentence by applyiong a sequence of reverse applications of productions.


## Grammar Transformations
### Transforming extend ENF grammar constructs
- Extended BNF include constructs for `optionality` and `repetition`.
- They are very convenient for clarity/conciseness of presentation of the grammar.
- However, they have to be removed, as they are not compatible with standard generative parsing techniques.

## Non-recursive ambiguity
- As the parse is essentially predictive, it cannot be faced with non-deterministic choices as to what rule to apply
- There might be sets of rules of the form: A -> alphaBeta1 | alphaBeta2 | alphaBeta3 | ...
- This would imply that the parser needs to make a choice between different right-hand sides that begin with the same symbol, which is not acceptable
- They can be eliminated using a factorization technique.

## Backtracking
- Backtracking is tricky and inefficient to implement
- In many cases, this eventually leads to a situation where most of the parsing time is spent on moves forward that are eventually backtracked
- Parsing with backtracking is seldom used
- The most simple solution is to eliminate the ambiguities from the grammar
- Some more elaborated solutions have been recently found that optimize backtracking that use a caching technique to reduce the number of generated sub-trees.

## Predictive parsing techniques
- Restriction: the parser must always be able to determine which of the right-hand sides to follow, only with its knowledge of the next token in input
- This essentially means top-down parsing without backtracking
- It is a form of deterministic parsing
- The assumption is that no backtracking is allowed/possible/neccessary

## Recursive descent predictive parser
- An implementation function is defined for each non-terminal symbol
- Its predictive nature allows it to always choose the correct right-hand-side
- The choice of right-hand side is coded in the function using if statements whose conditions use the FIRST and FOLLOW sets
- Each function choose the appropriate right-hand side and
    - matches terminal symbols with the consumed tokens
    - calls other functions to recognize other non-terminal symbols
- The parse tree is actually constructed by the nesting of the function calls, potentially recursively - hence the name of the technique
- Characteristics:
    - Easy to implement
    - Easy to trace/debug
    - Hard-coded: allows to handle unusual situations
    - Hard to maintain
    - Changes in the grammaer imply changes in the code

## Table-driven predictive parser
- A parsing table tells the parser which right-hand-side to choose
- The parsing algorithm is standard to all parsers
- Characteristics:
    - Easier to maintain: only the table changes when the language changes, the algorithm is universal.
    - Tracing/debugging is harder, as it is more abstract
    - If done manually, the process of building the parsing table is hard and error-prone for most languages
    - Tools can be used to generate the parsing table